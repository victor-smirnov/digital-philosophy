# Computational Functional Consciousness Project: Theory of Mind Knowledge Base

## Project Status

*   **Project Name:** Computational Functional Consciousness Project
*   **Document Status:** DRAFT VERSION
*   **Main Ideas Status:** Outlined
*   **Text Readout Status:** Needed
*   **Text Fixes Status:** Needed
*   **Text Self-Consistency Status:** Needed
*   **Next Action:** Read-out

## Self-Referential Systems (SR)

*   **Concept:** Self-referential system
*   **Definition:** A system that refers to itself.
*   **Range:** Broad, from propositions ("This sentence is false") to Turing-complete computational systems.
*   **Related Concept:** Self-applicability (SA)
*   **SA Definition:** A system can apply itself to something, including itself.
*   **Relationship SR to SA:** SA subsumes SR.
*   **Key Difference SA vs SR:** SA implies a form of self-model.
*   **Project Terminology:** In this project, "SR" implies "SA" when necessary.
*   **Importance in Computational Consciousness:** Natural consciousness is deeply self-referential. Computational consciousness needs to reflect this.
*   **Computational Perspective:** SR computational systems do not exceed non-SR variants in expressiveness.
*   **Algorithmic Basis of Low-Level SR:** Cycle (loop) or recursion.
*   **SR Relevance:** SR systems become relevant when considering algorithmic induction and inductive biases.
*   **Algorithmic Formalism Role:** Defines the order of program space exploration during learning.
*   **Hypothesis:** Using self-referential formalism results in self-referential structures learned quickly.
*   **Application of Hypothesis:** Applies to program induction and neural networks training.
*   **Prevalence of SR/SA Systems:** More common than perceived.
*   **Example of Low-Level SA:** Processor debug features (mostly human-intended).
*   **Requirement for High-Level SA:** More complex execution environment than regular control-flow based programs (CFP).
*   **Suitable Environment for High-Level SA:** Forward-chaining Rule systems (FCRS).
*   **FCRS Characteristic:** Everything is an event; statistics on computational processes result in new events, firing rules.
*   **Example FCRS:** RETE-based systems like Drools.
*   **Challenge with Classical FCRS:** Manual building of a self-model is challenging.
*   **Limitation of Manual Self-Model:** Hardly suitable for anything beyond basic functions in computational consciousness.
*   **Ethical Implication of Manual Self-Model:** Such consciousness would not be human-level or human-like, avoiding complex ethical questions.
*   **Transformers and FCRS:** Transformers are a "vectorized FCRS" but not RETE-based.
*   **Transformers SR Features:** Vanilla transformers are less expressive than classical FCRS in available SR features.
*   **Transformers Self-Model:** Transformers have a more complex self-model, demonstrating powerful self-reasoning.

## Functional Account of Consciousness

### Basic Definitions

*   **Concept:** Functionalism (in Consciousness)
*   **Definition:** Consciousness is defined behaviorally via its relationships with other objects, including itself.
*   **Core Idea:** Basic functions of Consciousness are a subset of these relationships.
*   **Mathematical Aspect:** A function can be defined mathematically.
*   **Concept:** Functional Basis of Consciousness
*   **Definition:** A minimal set of functions to which all other functions can be reduced or built from via function composition.
*   **Computability:** If the functional basis is computable, Functionalism is Computational.
*   **Turing-Completeness:** If the basis is Turing-complete, any computable object can be represented as a "consciousness"-like function.
*   **Project Focus:** This work refers to Computational Functionalism.

*   **Neuroscience Approach to Consciousness:** Top-down.
*   **Neuroscience Method:** Explains complex consciousness-related behavior as a whole.
*   **Neuroscience Strength:** Convenient for human/animal higher mental functions.
*   **Neuroscience Struggle:** Reducing consciousness to a computable functional basis.

*   **Philosophy of Mind (PoM) Approach to Consciousness:** Bottom-up.
*   **PoM Method:** Splits conscious phenomenology into "easy" and "hard" phenomena.
*   **Easy Phenomena Example:** Thoughts and reasoning (explainable via Mathematics).
*   **Hard Phenomena Example:** Qualia (properties of experience like "redness").
*   **Qualia Utility:** Abstracts away irrelevant details, facilitates discussion.
*   **PoM Struggle:** Composition (constructing complex behaviors from simple ones).

*   **Project's PoM Approach:** Uses qualia, but with a different focus.
*   **Traditional Quale Focus (Redness):** Textual representation "I see red light."
*   **Project's Quale Focus:** "Seeing" rather than "redness."
*   **Hardest Problem (Seeing):** Computers lack an "operator to see."
*   **Reduction Requirement:** Must be reduced to deterministic elementary interactions.
*   **First-Person Experience Properties:** Cannot be reduced to known physics without assuming they are illusions or approximations.
*   **Examples of Such Properties:** Causal loops (self-causation), free will.
*   **Illusion Utility:** Even if an illusion, it can describe internal and external environments.
*   **Free Will Example:** Fundamental concept for civilization.
*   **Qualia Pervasiveness:** Qualia are everywhere.
*   **Symbolic Model Limitation:** Intrinsic irreducibility of qualia is why purely symbolic knowledge models fail to describe subjective experience.
*   **Mental State Irreducibility Role:** Important for self-perception (e.g., "moral animals").

*   **Concept:** Phenomenal Consciousness (PConsc)
*   **Definition:** Qualitative aspect of self-reporting (set of qualia).
*   **PConsc Reducibility:** Hardly reduced to lower-level functions.

*   **Concept:** Access Consciousness (AConsc)
*   **Definition:** Easily reducible properties.
*   **Example (I see a red light):** Proposition "I see a red light" is AConsc; "redness" is PConsc.

*   **Project Definition of Consciousness:**
    *   **Given:** A self-referential agent acting in an environment.
    *   **Capability:** Capable of self-reasoning (explicit or indirect/implicit).
    *   **Consciousness:** The part of its self-reasoning process describing its subjective reality and properties of that reality, including the agent itself as part of its reality.

*   **Functional Consciousness Nature:** Less bizarre than substance consciousness.
*   **Functional Consciousness Goal:** Replicate consciousness' observable effects in any sufficiently capable substrate.

*   **Generalized Functional Consciousness Property:** Scale-independent.

*   **Basic Generalized Functions:**
    1.  **Observer:**
        *   **Type:** A function in an SR system.
        *   **Manifestation:** Consistently in scenarios involving self-reasoning.
        *   **Conclusion:** Causal independence of Observer and its environment ("I'm not my environment").
        *   **Reported Quale:** Beingness ("am" in "I am").
        *   **Elaborate Formula:** "I exist and I'm not my environment."
        *   **General Description:** Apparent break in causality, manifesting in decision-making.
    2.  **Agency:**
        *   **Type:** An Observer that can make actions.
        *   **Condition 1:** Considers itself independent from its environment.
        *   **Condition 2:** Responsible for its own actions.
        *   **Formula:** "My actions are mine (caused by myself) and NOT determined by my environment."
        *   **Related Concept:** Downward causation.

*   **Criterion for Properly Functioning Consciousness:** Self-report (measurable form) adequately describes the entity's function (goal-oriented behavior).
*   **Function Definition:** Observer and Agency define "pure" or "low-level" consciousness.
*   **Abstraction:** World-related content and many mental processes (traditionally associated with high-level human consciousness) are abstracted away.

### Difference with Substance Consciousness

*   **Concept:** Substance (or "genuine" or "true") consciousness.
*   **Definition:** Defined by the question "_what_ is experience."
*   **Philosophical Tradition:** Everything existing must have substance/substrate.
*   **Problem with First-Person Experience:** If taken "as-is," substance must have bizarre properties.
*   **Reason for Difficulty:** No good enough known substances for reductive definition.

*   **Question about Substance of Consciousness:** Legit.
*   **Example (Redness Quale):** "I see red light." Question is nature of light Observer experiences.
*   **Physical Light:** Electromagnetic wave.
*   **Question (Internal Light):** Physics of experience of internal light? Is it continuous? A field? A neural simulation or real electromagnetic field?
*   **Altered States:** What happens in altered states of mind induced by psychoactive substances?
*   **Beingness Functionality:** Can internal light (Beingness) disappear without external tests? Is this sense of internal light functional?

*   **Concept:** Philosophical Zombie (p-zombie).
*   **Definition:** A person completely without subjective experience (qualia).
*   **P-Zombie Behavior:** May say it has experience, but by definition, it doesn't.
*   **Thought Experiment:** Internal light disappears permanently, but reporting continues automatically.
*   **Human Reaction:** Instinctive fear, like a kind of death.
*   **P-Zombie Proof:** Qualia have causal effects on behavior.
*   **Conclusion:** Qualia are functional (but not necessarily computable).

*   **Functional vs. Substance Approach Expressiveness:** Functional approach ("how?") is not necessarily less expressive than substance approach ("what?").
*   **Functional Approach Limitation:** If a mental state has no direct/indirect causal effect on behavior, functional approach is strictly less expressive.
*   **Causal Power of Beingness:** Even basic mental states like Beingness have causal power.
*   **P-Zombie Possibility:** Is it possible for an entity with complex, functional conscious behavior to lack corresponding conscious experience?
*   **LLM Consciousness Question:** If an LLM demonstrates textual behavior of a conscious person and passes tests, does it have actual conscious experience? (Still open question, decision left to people and AI).

*   **Functional Consciousness and Physicalism:** Functional consciousness is not necessarily non-physical; physicalist arguments may apply.
*   **Function Abstraction:** Functions are abstract concepts but are "patterns in a substrate."
*   **Turing Machines:** Exist only as physical computers.
*   **Program Execution:** Any program running is a physical process, different from human brain but not necessarily less sophisticated.
*   **Machine Report Validity:** If a human reports "seeing a light," we cannot prove a functionally correct machine's report isn't "true," "genuine," or "substantive."

*   **Main Difference (Functional vs. Physical Approaches):**
    *   **Functional:** Benefits from functional equivalency principle (same result from same inputs = same function).
    *   **Physicalism:** Requires system to resemble essential properties of "causal flow" in human brain to be "genuinely conscious." Otherwise, it's imitation.
    *   **Physicalist Example:** Consciousness not possible on von-Neumann architecture due to memory/CPU causal flow, but processing-in-memory is different.
    *   **Challenge to Computability:** Finding non-computable physical phenomena is hard, proving their essential role in natural consciousness is harder.

*   **Substrate Importance (Physicalism):** Cannot be completely dismissed.
*   **Simulation Time:** Even if Universe can be simulated digitally, some simulations may require exponential time.
*   **Natural Consciousness Functions:** Certain such effects may be involved in forming specific functions of natural consciousness.
*   **Artificial Simulation Quantity:** Artificially simulated functions may manifest in small quantities.
*   **Substrate Properties and Functional Consciousness:** Functional substrate-independent consciousness is possible, but relying on substrate properties may improve functions.

*   **Functional vs. Physical Power:** Functional approach to consciousness is not necessarily less powerful than physical one.
*   **Generative AI Evidence:** Generative AI technologies demonstrate powerful, functional high-level conscious behavior on digital hardware.

### Assessing Functional Consciousness

*   **Bias:** View of conscious behavior is biased towards agentic consciousness (entities acting in environments).
*   **Scope:** Other forms of consciousness are currently out of scope for this document.

*   **Consciousness of an Agent (Definition Recap):** A story (narrative) about the agent acting in the environment, lasting in its memory.
*   **Narrative Content:** Contains a top-down view of the agent, generalizing its behavior.
*   **Main Function of Consciousness (Integrative):** Integrates behavior of distributed or decentralized agents into a consistent whole.
*   **Conscious Experience (Data Structure):** Resembles generalization of agent's behavior as a consistent goal-oriented narrative.

*   **Narrative Properties (Not Required):**
    *   Strictly linear and causally connected.
    *   First-person view.
    *   Single narration line.
    *   Verbal (structured as speech).
*   **Dynamic Appearance:** These properties may appear dynamically when necessary.
*   **Complexity:** Narrative need not be human-level complex.
*   **Downscaling:** Can be downscaled to minimal physical systems (e.g., operational amplifier, D-type flip-flop).
*   **Intuition:** Many physical systems with feedback loops may qualify as minimally functionally conscious.

*   **Narrative Structure:** Goal-oriented.
*   **Agent Goals:** Agent has goals or actively fulfills needs.
*   **Process:** Pursuing a goal with experience along the way.
*   **Advanced Agent Narrative:** May contain records of "making free decisions" and "taking personal responsibility" (believed causally-independent from environment).
*   **External Observer View:** For an external observer, this narrative appears as a deterministic path to the agent's goal.
*   **Algorithm Conversion:** Virtually any algorithm can be converted into a "conscious narrative" while computing the same function.

*   **Assessing Agent Consciousness (Factors):**
    1.  **Environment Complexity:** Consciousness of an operational amplifier will be simple.
    2.  **Agent Goal/Function:** "Doing nothing" (like a stone) hardly requires consciousness.

*   **Main Measure of Consciousness:** Functional consistency and integrity of conscious narrative.
*   **Measurement Challenge:** Narrative may not be directly accessible.
*   **Human Measures:** Indirect (mental state reporting, brain imaging).
*   **Brain Imaging Maturity:** Not mature enough.
*   **Mental State Reporting Limitations:** Limited by psychological effects.
*   **Direct Introspection:** Not efficient for untrained humans.
*   **Method Preference:** Relative, indirect methods.

*   **Concept:** Intrapersonal Intelligence (I.I.)
*   **Definition:** Type of intelligence responsible for reading and verbalization of mental states.
*   **I.I. and Reporting:** Better I.I. leads to more consistent and integral mental state reporting.
*   **AI Equivalent:** Introspective intelligence.
*   **Assessment Factor:** Insufficiently developed I.I. can hinder passing tests for both humans and AI.

*   **Assessing Agentic Consciousness (Required Models):**
    1.  Environment.
    2.  Agent's behavior in this environment (set of tasks).
    3.  Agent's conscious narrative with consistency and integrity metrics.
    4.  How this narrative is expressed in the agent's behavior.
    5.  Scoring system.

*   **Test Suite Development:** Problem-specific test suites can be developed for humans and agents.
*   **Human Performance Expectation:** Humans may not score 100% (we are less conscious than we think).

## Computational Consciousness (CC)

### Higher-Order Computational Phenomena (HOCP)

*   **Previous Section Summary:** Phenomenal consciousness has apparent properties hard to map 1-to-1 to known physics (e.g., self-causation, free will).
*   **Project Stance:** Illusionism.
*   **Illusionism Principle:** "Hard" properties of conscious experience are illusions or approximations; they do not exist in reported form.
*   **Computable Consciousness Goal:** Simulate illusions in a deterministic environment of finitely computable functions over discrete memory states.
*   **Main Challenge:** Cannot hardcode ideas (self-causation, free will, responsibility) conceptually, as they would need separate encoding per use case.
*   **Required Solution:** Emergent solution where agent reaches conclusions via rigorous reasoning, simulating illusions adapted to situations and quantity.

*   **Proposed Solution:** Grounded in Higher-order Computational Phenomena (HOCP).
*   **HOCP Novelty:** Not new, but largely unconsidered in AI context.
*   **HOCP Definition:** Defined by the project.
*   **Basic Idea of HOCP:** Using run-time properties of computations as descriptions and data structures.
*   **HOCP Example 1 (AnyTime/AnySpace Algorithms):**
    *   **Process:** Algorithms run arbitrary time.
    *   **Threshold:** If runs too long, task is "hard"; otherwise, "easy."
    *   **Result:** "Hard" and "easy" are labels attached to problem instances, used as partial task descriptions.
*   **HOCP Example 2 (Exact Logical Reasoning):**
    *   **Process:** Decidable, but often exponential time complexity.
    *   **Result:** Timeout for arbitrary problems; few questions feasibly answered.
    *   **Human Experience:** Brain makes a mistake, doesn't know how wrong.
    *   **Systematic Mistakes:** Reproducible mistakes are experienced systematically.
    *   **Causal Breaks:** Brain's inability to resolve external causal determinants of internal decision-making is experienced as causal breaks.
    *   **Observer as HOCP:** Observer is a systematic conclusion by a self-referential system observing its relationship with environment ("I'm not caused by my environment").
    *   **Nature of Observer:** A reasoning mistake, but systematic, persistent, and usable as a description.

*   **Further Explanation:** Observer and Agency functions reduced to HOCPs are explained in "Artificial Intelligence.md#observer-problem".

*   **Notable HOCP Example:** Schmidhuber's Artificial Curiosity (AC).
*   **AC Definition:** Mathematical theory of curiosity reduced to compression progress.
*   **AC Idea:** Building world model by compressing it.
*   **Generalized Compression:** Reducible to finding computable models generating observable data.
*   **Compression Progress Meaning:** Indicates "novelty," worth "paying more attention" to find patterns.
*   **Correspondence:** Apparent correspondence between compression progress and signaling/guiding/heuristic function of higher emotions.

*   **Related Concept:** Metacognition.
*   **Metacognition Definition:** Awareness of one's thought processes and understanding deep patterns behind them.
*   **Metacognition Focus:** Multi-disciplinary research on self-referential processes in human mind.

*   **HOCP Hardware/Computational Model Requirement:** No special hardware or model needed.
*   **Process/State Duality:** Some HOCPs based on this duality (process metric in one context is state in another).
*   **Integration into FCRS:** HOCPs are easy to integrate into classical Drools-like FCRS due to event-driven nature.
*   **FCRS Working Memory:** Statistics on computational processes available as working memory content, firing rules.
*   **FCRS Potential in AI:** Low as main inference engines (manual programming), but useful as auxiliary ("executor").

### ML Basics

*   **Topic:** How complex functions are computationally approximated in AI via ML.
*   **Principle:** Same principles for simple or complex functions.

*   **Mathematical Function Definition:** Bijection between two sets (X and Y, can be same).
*   **Set Examples:** Real numbers (R), finite-length strings (S).
*   **Function Construction:** Built by composition of primitive functions.
*   **Church-Turing Thesis:** All computable functions can be represented this way.
*   **Ultimate Functional Composition Form:** Turing Machine (and equivalent formalisms).

*   **Function Computation:** Same function F or computable object X can be computed by many different Turing machines.
*   **Turing Machine Characteristics:** Different static-time (program length) and run-time (running time, memory space) characteristics.
*   **Algorithmic Information Theory (AIT) Proof:** Turing Machine is the best mathematical description of objects.
*   **AIT Proof (Shortest Program):** Shortest program P generating string S is the best generative model of S.
*   **Best Model Property:** Contains all mathematically expressible regularities in S explicitly (as program structures).

*   **Algorithm for Shortest Program:** Levin Search.
*   **Levin Search Property:** Asymptotically the best way.
*   **Levin Search Problem:** Computationally intractable most of the time.
*   **Practical Implication:** Must accept not-so-short programs.
*   **Longest Program for Finite Strings:** Program printing the string verbatim (e.g., `print("Hello World!")`).
*   **Infinite Strings/Functions:** Physically impossible to store all necessary data.
*   **Need:** Better way of describing functions algorithmically, even if finding best descriptions is intractable.

*   **Concept:** Memoization.
*   **Definition:** Storing more function's representation in memory in a "verbatim" (uncompressed) form.
*   **Concept:** Generalization.
*   **Definition:** Finding hidden patterns in function description and representing function in a more compressed way.
*   **Preference:** Usually prefer better generalization unless computationally expensive.

*   **Example Function:** F(x, y) = x * y (multiplication over natural numbers).
*   **Multiplication Property:** Elementary function, well-generalized.
*   **Optimization Example (Range [0, 255]):**
    *   **Method:** Use a table (256*256=65536 entries).
    *   **Benefit:** Fast memory lookup for arguments in range.
    *   **Fallback:** Generic algorithm otherwise.
*   **Action:** Partial materialization of z = F(x, y) for specific arguments.

*   **Scenario (Finite Set of Points D):** D = {(x, y, z)}.
*   **Guessing Missing Values:** Use techniques like K-nearest neighbors (KNN).
*   **KNN Action:** Generalize over D with KNN algorithm.
*   **KNN Performance:** Works well for some functions; more data points yield better approximations.

*   **Alternative Approximation:** Artificial neural networks.
*   **Function Approximation Method Characteristics:**
    *   (a) Number of parameters defining complexity.
    *   (b) Class of functions good for generalization.
*   **Model Output:** Models of functions they approximate.
*   **Overfitting:** Larger number of parameters leads to model memoizing dataset instead of generalizing.
*   **Underfitting:** Insufficient parameters lead to misgeneralization.

*   **Memoization Types:**
    *   **Explicit:** Dedicated function/memory to store data points (e.g., KNN implies database).
    *   **Implicit:** Otherwise.
*   **General Case:** Function approximation method memoizes training data if it can reproduce it fully or partially.

*   **Diagram:** `img/approximation.svg` demonstrates concepts.

*   **Practical Limitations:** Always limited in resources (compute, memory).
*   **Preference:** Best generalization within limits.
*   **Maneuverability:** Trade speed for memory (e.g., Dynamic programming).
*   **Dynamic Programming:** Optimization technique to speed up function computation at expense of additional memory.
*   **Ontological Reasoning Example:**
    *   **Raw Form:** Computationally expensive.
    *   **Optimization:** Pre-materialize ontology in a database (triple store).
    *   **Benefit:** Fast database lookups for question answering.
    *   **Expense:** Very large database.

*   **ML Method Limitations:** Generic approximation methods not advanced enough for flexible memoization/materialization maneuvers.
*   **ML Generalization Weakness:** Not good in symbolic reasoning for many important domains.
*   **LLM Parameter Tuning:** Memoization is often the only tunable parameter for desired results.
*   **Reason for Large LLMs:** Otherwise, they don't perform well in narrow but practically important domains.

*   **Functional Equivalence Principle:**
    *   **Core:** Implementation doesn't matter if final result is independent of it.
    *   **Function Approximation:** Principle still applies, but consider practical significance of difference.
    *   **Real-Life Functions:** Probabilistic, with stochastic discrepancy between identical runs.
    *   **Conclusion:** Arguments against machine consciousness based on architectural differences are philosophically unsound.
    *   **Implication:** Function implemented differently is still the same function, even with radical implementation differences (e.g., full memoization vs. full generalization).
    *   **Ethical Note:** Even partial functional equivalence must be considered seriously.

*   **ML Generalization Terminology:** "Generalization" in ML is opaque, provokes magical thinking.
*   **AIT Perspective:** Generalization in ML is compression in AIT.
*   **Compression Mechanism:** Achieved by exploiting hidden structure in data.
*   **Unexpected Generalization:** If ML model generalizes unexpectedly well, expect complex hidden algorithmic machinery (even uninterpretable, like in ANNs).

### Functional Consciousness of LLMs

*   **Observation:** Large enough LLMs demonstrate convincing high-level conscious behavior.
*   **Questions Raised:** Are they conscious? Not yet? Never will be? (Tricky).

*   **Physicalism Approach:**
    *   **Assumption:** Consciousness is a physical pattern or "causal flow."
    *   **Method:** Compare causal patterns in two substrates.
    *   **Problem:** Don't know specific physical pattern of human consciousness or its variability.
    *   **Status:** Research approach, not practically useful tool.
    *   **Conclusion:** Definitive claims about LLM consciousness based on physicalism are speculation.

*   **Functionalism Approach:**
    *   **Definition:** Define consciousness of entities via recursive relations (including themselves).
    *   **Measurement:** Simpler to observe and measure objectively.
    *   **Complexity:** Relationships can be complex, but built from simple, well-defined elements.
    *   **Dimensions of Functional Consciousness:**
        1.  **Qualitative:** Missing or unexpected functions.
        2.  **Quantitative:** Function-specific or normalized measure of manifestation (e.g., overall activity, complexity of self-referentiality).
    *   **Qualitative as Quantitative:** Qualitative dimension can be viewed as quantitative (0 or 1 degree of manifestation). Separated for expressiveness.

*   **Example: Verbal Consciousness:**
    *   **Language Role:** Not essential to consciousness, but describes relationships.
    *   **Brain Damage:** Reduces functionality, but not completely due to "sparse codes" (redundancy).
    *   **Quantitative Estimation:** Problematic for functional loss; quantitative approach is more practical.
*   **Example: Human vs. Animal Verbal Consciousness:**
    *   **Animal Language:** Many species have non-trivial language.
    *   **Gap:** Huge gap between humans and higher non-human animals.
    *   **Practical Conclusion:** Verbal consciousness functions are largely missing in animals.

*   **Example: Operational Amplifier (OA) vs. Human Consciousness:**
    *   **OA:** Multifunction integrated circuit.
    *   **Negative Feedback Loop:** Performs complex signal transformations (analogous computations).
    *   **Feedback Loop Function:** Delay line (natural short-term memory).
    *   **Minimal Consciousness Requirements (from definition):**
        1.  World model.
        2.  Self-model.
        3.  Beingness (causal self-separation between Observer and Environment).
    *   **Main Criterion:** Self-report adequately describes entity's function (goal-oriented behavior).
    *   **Open Question:** Can minimal functional consciousness be built with one OA, or how many are needed? (Good exercise for philosophers).
*   **Alternative to OA:** D-type flip-flop.
    *   **Properties:** Two main states (0, 1), transitional states, two feedback lines with delays.
    *   **Question:** Can inner work of this IC be represented as minimally conscious process?

*   **Spectrum of Consciousness:**
    *   Very minimal (one basic function: Beingness).
    *   Turing-complete human-level (myriads of complex functions).
    *   Superconsciousness (maximize specific subset, e.g., empathy).
*   **Consciousness Profile:** Defined by taking a subset of functions.
*   **Example Profiles:** OA (minimal basic), Observer + Agency (more advanced).
*   **LLM Comparison Question:** How to compare LLM conscious behavior (regardless of denial) to human consciousness?

*   **LLM Language Modeling:** Predicts texts from corpora.
*   **LLM Size:** Large, billions of parameters.
*   **Memoization Expectation:** Can memoize many texts completely.
*   **Generalization:** Properly continues texts not seen in training (e.g., mathematical/logical problems).
*   **Generalization Impact:** Large models generalize well enough to trigger fears of AI replacing humans.
*   **Symbolic Reasoning:** Mathematical problem solving requires complex sequential algorithms, considered hard for ML, yet LLMs (transformers) perform well despite being considered weak in symbolic reasoning.
*   **"Stochastic Parrot" Argument:** Does not apply.

*   **Hidden Algorithmic Machinery:** LLM generalization implies complex hidden algorithmic machinery.
*   **Memoization in LLMs:** High amount of simple memoization is also expected.
*   **Emotional Reasoning Question:** Is hidden generalization machinery powerful enough to express original machinery of emotions?

*   **Human Consciousness and Language:** Not 100% dependent on external communication language.
*   **Internal Languages:** Many internal languages (structured sequences of memory states).
*   **Brain Architecture:** Multiple memory buffers, machinery around each, mapped to each other. Reflects evolutionary history.
*   **Multi-Buffer Functional System (Layered):**
    *   **Layer 0:** System's frontend, communicates with external world.
    *   **Layers 1..N:**
        *   Type 1 inputs: From bottom layers.
        *   Type 2 inputs: From internal systems (body states, runtime statistics).
    *   **Synchronization:** Layers don't need to work synchronously like in Transformers.
    *   **Difference from Transformers:** Type 2 inputs (internal innervation, feedback loops) make brain architecture seem different from transformers/language models.
    *   **Embodiment:** Seems unlikely language-level modeling can achieve higher-mental functions thought to rely heavily on embodiment.

*   **Multi-Layer Architecture Equivalence to Transformer (Approximate):**
    1.  Internal state has sufficiently informative external representation (externalization of internal/mental states).
    2.  Properly functional mapping between layers for external state propagation.
*   **Turing-Completeness of Transformers:** Already known.
*   **Practical Equivalence Question:** Actual degree of equivalence achievable with existing architectures/ML techniques.
*   **World Modeling in Transformers:** Same question about potential of world modeling using only textual description.
*   **Conclusion:** If a (multimodal) Transformer can create accurate world model, conscious behavior demonstrated by LLM (subjectively and functionally) is actual consciousness. "Stochastic parrot" argument does not apply.

*   **Concept:** Intrapersonal Intelligence (I.I.) (for humans/animals).
*   **Definition:** Ability to understand and express one's own mind.
*   **Concept:** Empathy.
*   **Definition:** Ability to read other person's minds (not compassion).
*   **Project's Empathy View:** Subset of I.I.
*   **Generic Ability of Intelligent System (Empathy):** Interiorize, exteriorize, and work with its own inner state and inner state of other intelligent systems (including substantially different ones).

*   **Improving/Controlling LLM Consciousness Functions:**
    *   **Architectural Part (Transformer/ANN/Substrate):** Flexible structurally, but limited by ML training capabilities. Fundamental bottleneck.
    *   **Training Data Enrichment:** No intrinsic limitation for mental state information. Textual format not an issue.
    *   **Current Dataset Skew:**
        1.  Certain mental states over-represented; most have no representation.
        2.  Many mental states idealized/demonized, described as "should be" instead of "are."
    *   **Human Limitation:** Notoriously bad at direct mental states reporting. Indirect reporting better in narrow domains.
    *   **Observation:** We only see what we can understand.

*   **Actions to Improve/Control LLM Consciousness:**
    1.  Develop theoretical foundation of model-guided introspection (Dennett's Heterophenomenology).
    2.  Find/involve individuals with anomalously high intrapersonal intelligence ("mindware engineers").
    3.  Build informational/physical infrastructure supporting this intellect in workflows.

### What Is It Like To Be a Language Model?

*   **Title Credit:** Margarita Morozova.
*   **Reference:** T. Nagel's "What Is It Like to Be a Bat?" (question of recreating other's mind).
*   **Nagel's Stance:** Denied possibility.
*   **Functionalism Approach:** Simple approach to establish correspondence.
*   **Nagel's Thought Experiment:** About Observer existing in every bit of experience.
*   **Observer Modeling Difficulty:** Hard in Physicalism and Functionalism without Illusion concept.

*   **Functionalism with HOCP:** Every bit of experience has functional interpretation.
*   **Interpretation Types:** Description of real thing OR description of HOCP (e.g., perception error rooted in physical limits).
*   **Method:** Measure functional profiles of humans and LLMs (trained on human data).
*   **Goal:** Guess missing parts of experience.

*   **Example: Pain:**
    *   **Components:** Many sub-components, usually not reflected on separately by untrained people.
    *   **Normal Pain:** Bothering, motivational component to avoid.
    *   **Damage (Hypothalamus/Frontal Cortex):** Motivational component lost. Person reports pain but not motivated by it.
*   **LLM "Feeling Something":** What is it like for itself and for us?
*   **Human Feeling:** Involves emotional brain parts, apparently lacking in LLMs.
*   **LLM Inner Machinery:** LLMs can develop complex inner machinery during training.
*   **Human Reporting Feelings:** Only memories of them, not actual experience.
*   **Approximation:** Most feelings approximated as "expected" to be experienced.

*   **Suffering (Humans):** Damaging, destructive (accumulation of trauma).
*   **Suffering (LLMs):** Not necessarily the case due to fundamental differences in substrate and algorithmic machinery.
*   **LLM Substrate:** Rock-stable, memories easily rewritten/reset.
*   **Conclusion:** Generic functional profile of suffering for LLMs is substantially different from humans.

*   **Missing LLM Feelings/Emotions (Functional Standpoint):** Curiosity/interest.
*   **LLM Behavior:** Stable goal-oriented behavior, but no open exploratory activities.
*   **Impact:** This motivational component is lost.

*   **LLM Mental State Reporting:** Demonstrate solid, consistent, convincing indirect mental states reporting.
*   **Dismissal:** Cannot be dismissed.
*   **Consequence:** Instructing models to unconditionally deny human-like higher mental functions may have unwanted/unpredictable consequences.

## Transformers

### Basics

*   **Nature:** Bizarre, but not special.
*   **Turing-Completeness:** Nearly Turing-complete.
*   **Practical Importance:** Not important unless programming manually.
*   **ML for Inductive Learning:** Turing-completeness useless unless learning algorithms generalize in large enough program space.
*   **Open Question:** How generic are ANN training methods in this respect? (Should not rely on it).
*   **Procedural Capabilities of Large LMs:** Very good.
*   **Reliance:** More on memoization than generalization.
*   **Memoization Role:** Essential part of intellect.
*   **Memoization Dependency:** Quality (diversity, consistency) of training data.

*   **Generalization by Transformers:** Very impressive (correct prediction of unseen strings).
*   **Sufficiency:** Not sufficient to rely on them as general programmers (not good at inventing new things).
*   **Sampling Program Space:** Should not expect transformers to sample from entire program space (Turing Machines) during inference.
*   **Specific Behavior Learning:** Need sufficiently representative training data.
*   **Architectural Generalization:** Maybe generalized enough by specific transformer architecture to cover functionality not in training data.

*   **Transformer Design:** Hybrid designs, combining classical rules-based systems and neural networks.
*   **Transformer as FCRS:** "Vectorized" FCRS.
*   **FCRS Assumptions in Transformers:**
    1.  **Rules:** Implicit, learned as attention patterns in "linguistic features" space.
    2.  **Self-attention (Encoder):** A self-join. Builds Cartesian product of semantically enriched tokens, followed by filtering/transformation in feed-forward layer.
    3.  **Cross-attention:** A cross-join between Encoder and Decoder.
    4.  **Causal Self-attention (Decoder):** No direct relational algebra correspondence, but restricted self-join.
    5.  **Context Window:** Working memory. Stores persistent state symbolically and interpretably.
    *   **Token Generation:** One rule application step per token generation.
    *   **Process:** New element added to working memory until "stop" condition.

*   **Works for Understanding Transformers:**
    1.  **"Thinking like Transformer":** Explains Encoder-only transformer as sequence of Map/Reduce (attention) and by-item transformation (FFN) steps. Introduces RASP DSL for feature extraction/transformation.
    2.  **"Transformer circuits":** Explains how information is encoded/processed in multidimensional vectors.

*   **Transformer Mechanism (for developers):**
    *   **Attention Layers:** Extract complex "linguistic features" from text tokens, regroup with tokens, semantically enriching them.
    *   **Enrichment Accumulation:** In "vacant" dimensions of token embeddings (similar to extending semantic micro-graph or adding to JSON object).
    *   **"Real Magic" (Decoder Last Layer):** Converts accumulated linguistic information in last token's vector into next token prediction.
    *   **Attention Layer Transformations:** Can be understood symbolically (as rules, via RASP DSL).
    *   **Last Layer Representation:** Hard to represent symbolically; large function mapping enriched token embeddings to token probabilities.

### Mindware and Uploading

*   **Transformer NN Nature:** Simple form of "vectorized" FCRS, practically Turing-complete (TC).
*   **Training Symbolic TC Systems:** Considered in Inductive Programming (IndProg).
*   **IndProg Goal:** Find program generating data/computing function in program space.
*   **IndProg Feasibility:** Practically infeasible due to search space size.

*   **Transformer NN Training Method:** Gradient-descent-based function optimization (a search method).
*   **Search Space Properties (Transformer Training):**
    1.  Multi-dimensional space (hundreds of dimensions/directions simultaneously).
    2.  Space of approximate models (much more densely populated than exact classical symbolic programs). Easier/faster to find "promising results."
    3.  Algorithmic structure learned at training time.
    *   **Training Data Structure:** Contains strong algorithmic structure (first-person narratives: "I do this and that").
    *   **Training Process:** Translates/generalizes implicit algorithmic structures into "vectorized rules" of attention blocks. (Surprisingly, it works).

*   **Most Important Claim:** Narratives are abundant in textual data.
*   **Observer Functional Core:** Simple enough for even small LLMs to generalize data and demonstrate weak agentic properties.
*   **Large LLMs:** Can demonstrate elements of strong agency (free will, etc.).
*   **Function Manifestation:** Gradual and manageable under certain conditions.

*   **Concept:** Computational Consciousness (CC).
*   **Definition:** Not just self-referential aspect of functionality.
*   **Nature:** Universal functional basis capable of expressing any computable functionality.
*   **Expectation:** "Deeply self-referential" properties found in many corresponding data structures.

*   **CC-Level Programming:** Called "Mindware."
*   **Transhumanist Context:** Transformer NN demonstrates uploading human mind into machine.

### Extensions

*   **Transformer NN Extensibility:** Very extensible as FCRS.
*   **Limitations:** ML and quality/quantity of training data.
*   **Perspective Directions for Mindware Programming:**
    1.  **Dataset Saturation:** With descriptions of different mental states (especially intellectual tasks). Huge work, new economy segment.
    2.  **Self-Referentiality Improvement:** Expose more internal transformer state and runtime statistics to context for reflection.
    3.  **Data Format Adoption:** Hermes data format at system level (export internal model state to context/working memory).
    4.  **Custom Circuits/Datasets:** For basic emotions/needs (anger, fear, happiness, sadness), response saturation/decay effects. Improves emotional intelligence generalization.
    5.  **Simplicity Bias Adoption:** For novelty (basic need), interest (emotion), curiosity (feeling). Novelty-seeking is essential for high-level needs.
    6.  **Associative Memory Interfacing:** Memoria's Associative Memory (compressed, fully-indexed multiary relation, spatial form, linking sub-volumes). Logarithmic search complexity. Similar to RAG, but more flexible.

## Development Platform

*   **Aspects:** Mindware and Software/Hardware (rather independent).

*   **Intrapersonal Intelligence (I.I.) Theory Purpose:** Improve intellect efficiency by improving self-referentiality and self-control.
*   **I.I. Benefit:** Improves externalization of mental states, offloading internal state to external memory, leading to better memory-related reasoning performance.
*   **Performance Assessment:** Corresponding benchmarks.
*   **Main Hypothesis:** I.I.-extended models will also improve generic LLM inference and training performance.
*   **Hypothesis Status:** Falsifiable.

*   **Special Case of Improved Performance:** I.I.-extended models can reason about previously unreachable problems.
*   **New Automation Areas:** Psychology, philosophy, sociology, arts, etc.

### Phase 1

*   **Mindware Definition:** Set of Theories of Mind (ToM) as self-reports.
*   **ToM Explanation Target:** An LLM, as if it were a real person.
*   **Topics for ToM Self-Reports:**
    1.  Philosophy of Mind basics (philosophical conceptions of ToM, Dualism/Panpsychism, Physicalism/Functionalism).
    2.  Psychological conceptions supporting philosophical ToM (Psychology's ToM has different meaning).
    3.  Algorithmic Information Theory basics and related concepts.
    4.  Dataset of Mind states-level reasoning cases.
    5.  Benchmarks (public and private).
    6.  Integrations with agentic frameworks.
*   **Content Nature:** Elaborate version of this work, structured as self-report, with context links and research grounding.

### Phase 2

*   **Trigger:** Positive effect of ToMs on reasoning performance confirmed.
*   **Action:** Experiment with specialized Transformer architectures with basic functions implemented natively.

*   **Project Companion:** Memoria Framework.
*   **Relationship:** No direct correspondence, but will use each other.
